{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sale Price Study \n",
    "\n",
    "\n",
    "## Inputs\n",
    "\n",
    "- outputs/datasets/datacollection/HousePrices.csv\n",
    "\n",
    "\n",
    "## Outputs\n",
    "\n",
    "- Start answering Business Requirement 1 and generate graphs.\n",
    "\n",
    "\n",
    "## Objectives\n",
    "\n",
    "- The objective here is to start addressing the business requirements. So my plan is to display how a houses attributes can influence the market value of said house.\n",
    "\n",
    "- Validate hypothesis. \n",
    "\n",
    "\n",
    "## CRISP-DM \n",
    "\n",
    "\"Data Understanding\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(os.path.dirname(current_dir))\n",
    "print(\"You set a new current directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()\n",
    "current_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = (pd.read_csv(\"outputs/datasets/datacollection/HousePrices.csv\"))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will now conduct an exploratory data analysis (EDA). This will give us a better insight into our DataFrame. \n",
    "\n",
    "import pandas_profiling\n",
    "pandas_report = pandas_profiling.ProfileReport(df, minimal=True)\n",
    "pandas_report.to_notebook_iframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation study\n",
    "\n",
    "- This is conducted to analyse missing data which will be potentially useful for the Data Cleaning step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_with_missing_data = df[df.columns[df.isna().sum() > 0 ]]\n",
    "vars_with_missing_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_var = vars_with_missing_data.select_dtypes(include=\"object\").columns.tolist()\n",
    "\n",
    "missing_var\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Create an imputer object by imputing the most frequest category\n",
    "categorical_imputer = SimpleImputer(strategy=\"most_frequent\")\n",
    "\n",
    "# Apply the imputer to the variable for missing values \n",
    "df[missing_var] = categorical_imputer.fit_transform(df[missing_var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[missing_var].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd \n",
    "\n",
    "# Select only object (categorical) columns for encoding\n",
    "\n",
    "categorical_cols = df.select_dtypes(include='object').columns\n",
    "encoder = OneHotEncoder(sparse=False, drop=None)\n",
    "encoded_array = encoder.fit_transform(df[categorical_cols])\n",
    "encoded_df = pd.DataFrame(encoded_array, columns=encoder.get_feature_names_out(categorical_cols))\n",
    "df_ohe = pd.concat([df.drop(columns=categorical_cols), encoded_df], axis=1)\n",
    "\n",
    "# Check the new dataframe's shape and look at the first few rows\n",
    "print(df_ohe.shape)\n",
    "df_ohe.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigating correlation\n",
    "\n",
    "corr_spearman = df_ohe.corr(method='spearman')\n",
    "corr_spearman_saleprice = corr_spearman['SalePrice'].copy()\n",
    "corr_spearman_sorted = corr_spearman_saleprice.reindex(corr_spearman_saleprice.abs().sort_values(ascending=False).index)\n",
    "top_10_corr_spearman = corr_spearman_sorted[1:11]\n",
    "top_10_corr_spearman"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "corr_pearson = df_ohe.corr(method='pearson')\n",
    "corr_pearson_saleprice = corr_pearson['SalePrice'].copy()\n",
    "corr_pearson_sorted = corr_pearson_saleprice.reindex(corr_pearson_saleprice.abs().sort_values(ascending=False).index)\n",
    "top_10_corr_pearson = corr_pearson_sorted[1:11]\n",
    "top_10_corr_pearson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we will delve deeper into the correlations\n",
    "\n",
    "top_n = 5\n",
    "\n",
    "# This code is not mine. It's taken from the Churnometer walkthrough project. \n",
    "\n",
    "set(corr_pearson[:top_n].index.to_list() + corr_spearman[:top_n].index.to_list())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_analyze = ['1stFlrSF', '2ndFlrSF', 'BedroomAbvGr', 'BsmtFinSF1', 'BsmtUnfSF']\n",
    "features_to_analyze "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eda = df.filter(features_to_analyze + [\"SalePrice\"]).copy()\n",
    "df_eda.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from feature_engine.discretisation import EqualFrequencyDiscretiser\n",
    "discretiser = EqualFrequencyDiscretiser(q=6, variables=[\"SalePrice\"])\n",
    "df_eda_transformed = discretiser.fit_transform(df_eda)\n",
    "df_eda_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates bins or intervals.\n",
    "\n",
    "discretiser.binner_dict_['SalePrice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are making labels. \n",
    "\n",
    "labels = discretiser.binner_dict_[\"SalePrice\"]\n",
    "n_factor = len(labels) - 1 \n",
    "\n",
    "labels_map = {\n",
    "    n: (\n",
    "        f\"< {labels[1]}\" if n == 0 else\n",
    "        f\"+{labels[n]}\" if n < n_factor -1 else\n",
    "        f\"{labels[n]} to - {labels[n + 1]}\"\n",
    "    )\n",
    "    for n in range(n_factor)\n",
    "}\n",
    "\n",
    "labels_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Any unmapped values stay as they are.\n",
    "\n",
    "df_eda[\"SalePrice\"] = df_eda[\"SalePrice\"].map(labels_map).fillna(df_eda[\"SalePrice\"])\n",
    "df_eda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we will use the Seaborn library for data visualization. Hue variable will be used to color data points by a categorical variable. \n",
    "\n",
    "hue_order = [label for label in labels_map.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from pandas.api.types import is_numeric_dtype\n",
    "\n",
    "# Set seaborn style for plots\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Custom intervals\n",
    "intervals = ['< 118500.0', '+118500.0', '+139700.0', '+163000.0', '+190000.0', '241416.66666666663 to - inf']\n",
    "bin_edges = [-float('inf'), 118500.0, 139700.0, 163000.0, 190000.0, 241416.66666666663, float('inf')]\n",
    "\n",
    "# Function to plot numerical columns against target variables\n",
    "def plot_numerical_distribution(df, column, target_var, hue_order):\n",
    "    if is_numeric_dtype(df[column]):\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "        sns.histplot(df, x=column, hue=target_var, hue_order=hue_order, kde=True, element=\"step\", ax=ax)\n",
    "        handles, labels = ax.get_legend_handles_labels()\n",
    "        ax.legend(labels=labels, title=target_var)\n",
    "        ax.set_title(f\"{column} Distribution\", fontsize=15)\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"Column '{column}' is not numerical and will be plotted.\")\n",
    "\n",
    "# Define the target variable and columns to plot\n",
    "target_var = \"SalePrice\"\n",
    "numeric_columns = [\"1stFlrSF\", \"2ndFlrSF\", \"BedroomAbvGr\", \"BsmtFinSF1\", \"BsmtUnfSF\"]\n",
    "\n",
    "\n",
    "# Iterate over the selected columns and plot\n",
    "for column in numeric_columns:\n",
    "    plot_numerical_distribution(df_eda_transformed, column, target_var, hue_order=intervals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note for the assessors: I tried for about 2/3 days to try and get my intervals into the legend and get them color coded but I just could not find the solution. I tried Slack, Stack Overflow among other resources and never found a solution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the two columens into a separate DataFrame\n",
    "df_selected = df[[\"SalePrice\", \"BedroomAbvGr\"]]\n",
    "\n",
    "# Calculate the correlation matrix using Pearson method\n",
    "correlation_matrix = df_selected.corr(method=\"pearson\")\n",
    "# Extract the specific correlation value\n",
    "df_pearson = correlation_matrix.at[\"SalePrice\", \"BedroomAbvGr\"]\n",
    "df_pearson\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df[\"BedroomAbvGr\"]\n",
    "y = df[\"SalePrice\"]\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.scatterplot(x=x, y=y)\n",
    "\n",
    "plt.xlabel(\"BedroomAbvGr\")\n",
    "plt.ylabel(\"SalePrice\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- My first hypothesis was that houses with 4 bedrooms appear to reach the highest prices. We can clearly see on my scatterplot that this is the case. We can clearly see a number of properties that reach the $500,000+. Some properties even exceed $700,000. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reusing the same code above for our next graph. \n",
    "\n",
    "# Extract the two columens into a separate DataFrame\n",
    "df_selected = df[[\"SalePrice\", \"BsmtUnfSF\"]]\n",
    "\n",
    "# Calculate the correlation matrix using Pearson method\n",
    "correlation_matrix = df_selected.corr(method=\"pearson\")\n",
    "# Extract the specific correlation value\n",
    "df_pearson = correlation_matrix.at[\"SalePrice\", \"BsmtUnfSF\"]\n",
    "df_pearson\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df[\"BsmtUnfSF\"]\n",
    "y = df[\"SalePrice\"]\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.scatterplot(x=x, y=y)\n",
    "\n",
    "plt.xlabel(\"BsmtUnfSF\")\n",
    "plt.ylabel(\"SalePrice\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For my second hypothesis, I argued that homes with smaller unfinished basements tend to have a wider ranger of sale prices. While there is a cluster of properties between 0-1000 square feet range, I can see that the properties are evenly distributed here across a range of prices ranges and square footage. There is a couple of outliers in the $700,000 price range also which may suggest that other factors have come into play of influencing the house prices. The correlation is not as strong here as a house with 2000+ unfinished sf can be seen to have the same price as a house with 0 - 500 unfinished sf. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reusing the same code above for our next graph. \n",
    "\n",
    "# Extract the two columens into a separate DataFrame\n",
    "df_selected = df[[\"SalePrice\", \"1stFlrSF\"]]\n",
    "\n",
    "# Calculate the correlation matrix using Pearson method\n",
    "correlation_matrix = df_selected.corr(method=\"pearson\")\n",
    "# Extract the specific correlation value\n",
    "df_pearson = correlation_matrix.at[\"SalePrice\", \"1stFlrSF\"]\n",
    "df_pearson\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df[\"1stFlrSF\"]\n",
    "y = df[\"SalePrice\"]\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.scatterplot(x=x, y=y)\n",
    "\n",
    "plt.xlabel(\"1stFlrSF\")\n",
    "plt.ylabel(\"SalePrice\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- For my third hypothesis, I argued that houses with larger amounts of square footage on the first floor generally cost more than houses with smaller amounts of square footage. In the scatterplot, we can see this is the case. When the 1st floor square foot value increases we can see that the price also increases. The bar graph also shows this to be true. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final thoughts\n",
    "\n",
    "From the above analysis and graphs, I've come to the conclusion that houses with more bedrooms, larger area and more work completed tend to generate a higher sale price, as you would expect. \n",
    "\n",
    "Diving deeper into this, we can see houses with 4 bedrooms appear to reach the highest prices which relates Hypothesis 1. The data shows that 4-bedroom homes tend to reach higher prices, with some homes exceeding $500,000 and even surpassing $700,000. Then we see, homes with smaller unfinished basements have a wider range of sale prices which relates to Hypothesis 2. Although there's a cluster of properties with unfinished basements between 0-1000 sq. ft, the price range is broad. As mentioned in previous cells, outliers with higher prices suggest other factors like location may influence prices. The correlation between unfinished basement and price is weak indicating that this not might be used to strongly determine the sale price of a house. Finally, houses with larger first-floor square footage tend to cost more. The analysis shows a positive correlation between first-floor square footage and price, with larger homes generally costing more. This is evident in both the scatterplots and bar graphs. \n",
    "\n",
    "The above information and analysis shows that larger homes with 4 bedrooms and more first-floor space tend to command higher prices. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
